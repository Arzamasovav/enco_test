# Тестовое задание для ЭНКО

Для облегчения тестирования в репозитории есть docker контейнеры, которые запускают все необходимые сервисы. Для
их запуска необходимо иметь установленный docker. Запуск контейнеров:
```
make up
```

Также, в случае запуска с контейнерами, необходимо поместить тестовый датасет в корень репозитория с названием 
`test_dataset.csv`

## Задача №1

Результат находится в файле `task_1.ipynb`.

### Подготовка к выполнению

В файле `task_1.ipynb`, находится переменная `DATASET_PATH` - это путь к тестовому датасету. Нужно либо в ней указать
актуальный путь к датасету на вашем устройстве, либо поместить датасет в корень репозитория, чтобы дефолтный путь стал
актуальным.

## Задача №2

Результат находится в папке `dags`, итог выполнения можно посмотреть в БД контейнера (или другой БД, адрес которой
указан при запуске), в `postgres > enco-test > task_2 > test_dataset`

### Подготовка к выполнению

В случае с запуском в контейнерах ничего подготавливать не нужно. В ином случае необходимо:
- Указать переменные для доступа к БД и S3 хранилищу в файле `dags > task_2 > config.py`
- Указать переменные пути к датасету\скачиваемому файлу в `dags > task_2 > task.py`
- Поместить даг и зависимые файлы в среду выполнения, установить необходимые зависимости

### Выполнение

В случае запуска в контейнере:
- Зайти в airflow: http://0.0.0.0:8080/, логин и пароль будут указаны в консоли контейнера `airflow-web-server`
после запуска, строкой типа `Simple auth manager | Password for user 'admin': rd33vfq2KNam5xDa`
- Запустить даг

## Задача №3

Результат находится в файле `task_3.sql`, итог выполнения можно посмотреть в БД контейнера (или другой БД, в которой был
запушен скрипт), в `postgres > enco-test > task_3`.

### Выполнение

В случае запуска в контейнере необходимо выполнить команду:
```
make task-3
```

В ином случае запуск на ваше усмотрение, единственное, что нужно указать - путь до тестового датасета в файле
task_3.sql, в строке 
```
COPY task_3.raw_downloads FROM '/opt/app/test_dataset.csv' WITH (FORMAT csv, HEADER true, DELIMITER '|');
```

## Задача №4

Результат находится в файле `task_4.sql`, итог выполнения можно посмотреть в БД контейнера (или другой БД, в которой был
запущен скрипт), в `clickhouse > enco-test > views > postgres_clickhouse_view`.

### Выполнение

В случае запуска в контейнере необходимо:
- Если задача №3 уже выполнялась и контейнеры не были перезапущены или удалены и снова запущены, то данные в БД уже
есть, необходимо только построить view.
```
make task-4
```
- В ином случае нужно сначала наполнить данными postgres, а потом построить view.
```
make task-3
make task-4
```

В ином случае запуск на ваше усмотрение, но нужно будет заменить параметры подключения к таблицам postgres, в файле
task_3.sql. Пример: 
```
FROM postgresql(
'postgres:5432',
'enco_test',
'downloads',
'postgres',
'postgres',
'task_3'
) AS downloads

Заменить на

FROM postgresql(
'{хост вашей БД}:{порт вашей БД}',
'{название вашей БД}',
'{название вашей таблицы}',
'{логин, под которым можно подключиться к БД}',
'{пароль, с которым можно подключиться к БД}',
'{название вашей схемы}'
) AS downloads
```
